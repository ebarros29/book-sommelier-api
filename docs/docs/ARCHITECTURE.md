# Documenta√ß√£o da Arquitetura

Este documento descreve a arquitetura do projeto **Book Recommendation Data API**, focando nas decis√µes de arquitetura, no fluxo de dados ponta a ponta, na prepara√ß√£o para escalabilidade futura e na integra√ß√£o com modelos de Machine Learning.

## 1. üåê Vis√£o Geral
Book Sommelier API √© uma plataforma constru√≠da com Python e Flask que realiza:
1. Coleta de dados por Web Scraping
2. Armazenamento dos livros em CSV e banco de dados
3. Exposi√ß√£o de uma API REST para consultas e an√°lises
4. Prepara√ß√£o para pipelines de Machine Learning (recomenda√ß√µes, an√°lises de pre√ßo, NLP etc.)

O projeto segue uma arquitetura organizada em Pipeline ‚Üí API ‚Üí Consumo permitindo f√°cil evolu√ß√£o e escalabilidade.

## 2. üß© Pipeline: Ingest√£o ‚Üí Processamento ‚Üí API ‚Üí Consumo
O pipeline do Book Sommelier API garante a coleta, padroniza√ß√£o, armazenamento e disponibiliza√ß√£o dos dados de livros para aplica√ß√µes, an√°lises e modelos de machine learning. Ele √© composto por quatro etapas, cada uma com responsabilidades bem definidas, componentes espec√≠ficos e padr√µes arquiteturais pr√≥prios.
A seguir, cada etapa √© descrita com o fluxo de dados.

üîµ 2.1 Ingest√£o ‚Äî Web Scraper (scripts/scraper.py)

A etapa de ingest√£o √© respons√°vel por capturar dados brutos diretamente da fonte externa (books.toscrape.com). Este √© o ponto inicial do pipeline.

üéØ Responsabilidades da etapa de ingest√£o

O BookScraper executa um fluxo completo de extra√ß√£o:

*Descoberta da pagina√ß√£o*

- Acessa a primeira p√°gina do cat√°logo (page-1.html)
- Identifica o total de p√°ginas via HTML (ul.pager)
- Registra last_page para controlar o loop de scraping

Isso permite navegar de forma din√¢mica mesmo se o site aumentar/deixar de exibir produtos.

*Coleta de URLs individuais de livros*

Para cada p√°gina:
- Envia requisi√ß√£o HTTP com cabe√ßalho (User-Agent)
- Faz parsing do HTML com BeautifulSoup
- Extrai links relativos (href) de cada <article class="product_pod">

O resultado √© uma lista _books_urls com todos os livros do site.

*Extra√ß√£o de metadados de cada livro*

Para cada URL individual, realiza: 
- Leitura da p√°gina detalhada do livro
- Extra√ß√£o dos campos: T√≠tulo, Pre√ßo bruto, Moeda, Pre√ßo em centavos, RatingClasse, Categoria, URL da Imagem.

*Padroniza√ß√£o e limpeza dos dados*

Antes da persist√™ncia:
- Convers√£o de moeda ‚Üí currency code internacional
- Convers√£o de pre√ßos ‚Üí inteiros em centavos
- Tratamento de campos opcionais (rating, img_url, category)
- Garantia de limpeza (strip(), valida√ß√µes b√°sicas)

*Persist√™ncia via DataStorage*

O scraper √© projetado para funcionar com qualquer mecanismo de persist√™ncia, por meio de uma interface externa (DataStorage). 
No projeto atual, usamos CSVWriter. 
Grava os dados estruturados em: /data/books.csv.

O CSV ser√° o dataset usado pelas etapas posteriores.

Tecnologias utilizadas
- Requests ‚Üí comunica√ß√£o HTTP
- BeautifulSoup ‚Üí parsing de HTML
- Regex ‚Üí normaliza√ß√£o de pre√ßo/moeda
- Pandas (indiretamente) ‚Üí posteriormente usado na leitura do CSV

Resultado da Etapa

üìÑ data/books.csv

## 3. üß¨ Arquitetura de Escalabilidade Futura

A arquitetura do Book Sommelier API foi projetada pensando n√£o apenas no funcionamento atual, mas tamb√©m em sua evolu√ß√£o natural √† medida que o volume de dados cresce, novos requisitos surgem e aplica√ß√µes de machine learning passam a consumir os dados disponibilizados. O sistema adota uma postura modular, permitindo que cada parte da solu√ß√£o possa ser substitu√≠da, ampliada ou reorganizada sem a necessidade de reescrever o projeto do zero.

A seguir, descrevemos cada eixo de escalabilidade previsto no desenho arquitetural.

3.1 Escala Horizontal da API

A camada de API foi pensada para suportar o modelo de escalabilidade horizontal, no qual m√∫ltiplas inst√¢ncias da aplica√ß√£o rodam simultaneamente para absorver picos de demanda.
Isso se torna poss√≠vel gra√ßas ao empacotamento via Docker, que permite facilmente replicar o container da aplica√ß√£o e execut√°-lo em diferentes provedores compat√≠veis com containers, como Render, Railway, Fly.io ou Kubernetes.
Uma vez que m√∫ltiplas inst√¢ncias estejam rodando, um load balancer pode distribuir requisi√ß√µes entre elas de maneira uniforme, garantindo:
- maior toler√¢ncia a falhas,
- maior disponibilidade,
- melhor desempenho,
- isolamento de workloads pesadas.

A arquitetura tamb√©m permite configurar workers independentes para dividir tarefas de forma eficiente ‚Äî por exemplo, servir requisi√ß√µes s√≠ncronas enquanto processos mais caros s√£o tratados em paralelo.

3.2 Escalonamento do Pipeline

Hoje, o fluxo do pipeline √© simples e sequencial: scraping ‚Üí CSV ‚Üí importa√ß√£o para o banco.
Essa estrutura √© eficiente para pequenas e m√©dias quantidades de dados, mas pode se tornar um gargalo √† medida que novas fontes, volumes maiores ou m√∫ltiplos scrapers forem adicionados.
Para o futuro, o pipeline pode ser evolu√≠do para um modelo distribu√≠do e altamente escal√°vel:
Scraper ‚Üí FILA (Kafka/RabbitMQ) ‚Üí Processador paralelo ‚Üí Banco ‚Üí API ‚Üí ML

Nesse formato:
O Scraper envia mensagens para um barramento ass√≠ncrono (Kafka, RabbitMQ).
Consumidores paralelos processam cada item individualmente, permitindo ingest√£o massiva.
O banco √© alimentado em fluxo cont√≠nuo.
A API passa a servir dados atualizados em tempo real.
Pipelines de ML recebem informa√ß√µes renovadas automaticamente.

Esse desenho suporta m√∫ltiplos scrapers, maior throughput e velocidades de ingest√£o muito superiores, deixando o sistema pronto para cen√°rios de Big Data.

3.3 Migra√ß√£o de CSV para Data Lake

Embora o CSV seja √≥timo para prototipagem, ele n√£o escala bem quando surgem demandas relacionadas a:
- alta volumetria,
- versionamento de dados,
- consultas avan√ßadas,
- integra√ß√£o com pipelines anal√≠ticos.

Por isso, prev√™-se uma futura migra√ß√£o para formatos mais robustos como:
- Parquet (compacto, colunar, otimizado),
- ORC, ou mesmo para armazenamento distribu√≠do como:
a. Amazon S3,
b. Google Cloud Storage,
c. MinIO (self-hosted S3).

Com isso, ferramentas como AWS Glue, Apache Spark ou Databricks podem processar grandes volumes em segundos, dando ao projeto um caminho claro para an√°lises avan√ßadas e engenharia de caracter√≠sticas para ML.

3.4 Cache em Camadas

Para evitar reprocessar consultas que mudam pouco (como top-rated, categorias ou estat√≠sticas de overview), o sistema pode adicionar uma camada de cache como o Redis.
Isso permite:
- respostas quase instant√¢neas (milissegundos),
- menor carga no banco,
- escalabilidade mais barata,
- √≥tima performance para dashboards e consumidores repetitivos.

3.5 Observabilidade Completa

√Ä medida que a aplica√ß√£o cresce, torna-se essencial ter visibilidade clara de seu comportamento. A arquitetura prev√™ adi√ß√£o de: 
- logs estruturados em JSON (para an√°lise no ELK/Datadog),
- Prometheus para coletar m√©tricas (lat√™ncia, erros, throughput),
- Grafana para dashboards e alertas,
- OpenTelemetry para rastreamento distribu√≠do entre API, banco, scrapers e pipelines.

Com isso, qualquer anomalia √© detectada rapidamente e as equipes conseguem observar o impacto de novas vers√µes ou cargas elevadas.

## 4. üß† Cen√°rio Real de Uso para Data Science & ML

A arquitetura foi desenhada para servir como uma base s√≥lida para pipelines de machine learning. Ela disponibiliza dados limpos, consistentes e padronizados, facilitando tarefas como modelagem, previs√£o e gera√ß√£o de recomenda√ß√µes.
A seguir est√£o cen√°rios reais de uso contemplados pela arquitetura.

4.1 Regress√£o de Pre√ßo

A API permite a cria√ß√£o de modelos que estimam o valor de um livro com base em m√∫ltiplas vari√°veis.
Um cientista de dados pode usar atributos como: rating num√©rico, categoria, tokens do t√≠tulo extra√≠dos (TF-IDF, Bag-of-Words), caracter√≠sticas da imagem de capa extra√≠das via CNNs, para treinar regress√µes ou modelos de boosting capazes de prever valor aproximado de um item.

4.2 Sistemas de Recomenda√ß√£o

Com os dados de categoria, t√≠tulo e metadados, o sistema suporta recomenda√ß√µes usando:
- similaridade textual entre t√≠tulos (cosine similarity),
- embeddings NLP gerados por BERT, Sentence-BERT ou Word2Vec,
- categorias correlacionadas,
- m√©todos colaborativos como LightFM ou matrix factorization.

Isso permite construir um ‚ÄúSommelier de Livros‚Äù, sugerindo ao usu√°rio obras que combinam com seu gosto.

4.3 An√°lise de Mercado

A camada de insights exp√µe informa√ß√µes prontas para:
- comparar pre√ßos por categoria,
- entender a distribui√ß√£o de avalia√ß√µes,
- identificar livros premium vs populares,
- acompanhar varia√ß√µes temporalizadas do cat√°logo (futuro).

Esses dados podem alimentar dashboards ou an√°lises explorat√≥rias.

4.4 API de Feature Store

A arquitetura prev√™ a cria√ß√£o de endpoints futuros como:
/api/v2/features/book/<id>

Essa API entregaria vetores de caracter√≠sticas pr√©-calculadas (ex.: embedding do t√≠tulo, categoria one-hot, pre√ßo normalizado).
Tais vetores podem ser diretamente consumidos por modelos de ML, economizando tempo e padronizando o fluxo.

## 5. ü§ñ Plano de Integra√ß√£o com Modelos de ML
A arquitetura prev√™ quatro est√°gios de maturidade para integra√ß√£o de machine learning:

5.1 Est√°gio 1 ‚Äî Prepara√ß√£o dos Dados

Criar um diret√≥rio /ml e consolidar datasets com:
Pythonimport pandas as pddf = pd.read_csv("data/books.csv")Mostrar mais linhas
Aqui acontecem:
- limpeza adicional,
- gera√ß√£o de features,
- splits de treino/valida√ß√£o.


5.2 Est√°gio 2 ‚Äî Treinamento

Modelos recomendados:
- RandomForest ‚Äî f√°cil de treinar, √≥timo baseline para regress√£o.
- XGBoost/LightGBM ‚Äî alta performance para ranking e regress√£o.
- BERT/Sentence-BERT ‚Äî representa√ß√µes sem√¢nticas para t√≠tulos.
- LightFM ‚Äî recomenda√ß√£o colaborativa.


5.3 Est√°gio 3 ‚Äî Deploy de Modelos na API

A API pode ganhar endpoints como:
POST /api/v2/predictions/price
POST /api/v2/recommendations

Nesses endpoints, o modelo carregado em mem√≥ria recebe features e devolve a infer√™ncia.

5.4 Est√°gio 4 ‚Äî MLOps

Para produ√ß√£o, o ciclo completo incluiria:
- monitoramento de drift,
- versionamento de modelos,
- benchmark autom√°tico,
- reprocessamento de dados,
- Feature Store compartilhada.

## 6. üì¶ Componentes Cr√≠ticos do Sistema
O BookScraper √© respons√°vel pela coleta de p√°ginas e extra√ß√£o dos campos brutos. O CSVWriter atua como camada de persist√™ncia intermedi√°ria, gravando os dados em CSV. O BookImportService realiza a leitura do arquivo, normaliza dados e evita duplicidades antes de inserir no banco. O BookRepository fornece abstra√ß√µes de acesso ao banco via SQLAlchemy, enquanto o Modelo Book representa a entidade persistida. O Books Blueprint exp√µe a API p√∫blica principal e o Insights Blueprint concentra endpoints anal√≠ticos em Pandas. O Dockerfile empacota a aplica√ß√£o como container, e o docker-compose orquestra o ambiente com API e Postgres. Por fim, o Alembic controla a evolu√ß√£o do schema do banco.

## 7. üìö Tecnologias e Decis√µes Arquiteturais
A solu√ß√£o utiliza Python, Flask com Blueprints, SQLAlchemy com PostgreSQL, Requests e BeautifulSoup para scraping, Pandas para an√°lises, Docker e docker-compose para infraestrutura e Alembic para migra√ß√µes. O CSV funciona como fonte intermedi√°ria simples e eficiente.

## 8. üßæ Conclus√£o
A arquitetura do Book Sommelier API √© modular, extens√≠vel e preparada para o futuro. Ela separa claramente scraping, processamento, API e an√°lise, servindo tanto aplica√ß√µes quanto cientistas de dados. O design facilita escalabilidade, adi√ß√£o de novos scrapers, cria√ß√£o de novas features e integra√ß√£o com pipelines de machine learning e MLOps.



